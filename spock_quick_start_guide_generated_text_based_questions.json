{"output_questions": [{"QUESTION": "What happened to the Spock Early Access System on March 15, 2023?", "ANSWER": "The Spock Early Access System was decommissioned on March 15, 2023. However, the file systems that were available on Spock are still accessible from the Home server and the Data Transfer Nodes (DTN), ensuring that all data remains accessible."}, {"QUESTION": "Who were the primary users of the Spock system?", "ANSWER": "The primary users of the Spock system included teams from the Center for Accelerated Application Readiness (CAAR), Exascale Computing Project (ECP), NCCS staff, and vendor partners. It served as an early-access testbed for these groups."}, {"QUESTION": "Describe the hardware configuration of a Spock compute node.", "ANSWER": "Each Spock compute node consisted of a 64-core AMD EPYC 7662 'Rome' CPU with 2 hardware threads per physical core, 256 GB of DDR4 memory, and was connected to 4 AMD MI100 GPUs via PCIe Gen4. It also had 2 NVMe devices with sequential read and write speeds of 6900 MB/s and 4200 MB/s, respectively."}, {"QUESTION": "Explain the NUMA domain configuration on a Spock compute node.", "ANSWER": "There are 4 NUMA domains per Spock compute node, defined as follows: NUMA 0 includes hardware threads 000-015 and 064-079 with GPU 0, NUMA 1 includes hardware threads 016-031 and 080-095 with GPU 1, NUMA 2 includes hardware threads 032-047 and 096-111 with GPU 2, and NUMA 3 includes hardware threads 048-063 and 112-127 with GPU 3."}, {"QUESTION": "What is the total number of GPUs in the Spock system, and what are their capabilities?", "ANSWER": "The Spock system contains a total of 144 AMD MI100 GPUs. Each GPU has a peak performance of up to 11.5 TFLOPS in double-precision for modeling and simulation, and up to 184.6 TFLOPS in half-precision for machine learning and data analytics. Each GPU contains 120 compute units and 32 GB of high-bandwidth memory (HBM2) with access speeds of up to 1.2 TB/s."}, {"QUESTION": "How can users access data previously stored on Spock after its decommissioning?", "ANSWER": "Users can access data previously stored on Spock from the Home server and the Data Transfer Nodes (DTN). If users do not have access to other OLCF systems, their project will move to data-only status for 30 days. For further assistance, users can contact help@olcf.ornl.gov."}, {"QUESTION": "What role did Spock play in relation to the Frontier system?", "ANSWER": "Spock was used as an early-access testbed that contained similar hardware and software as the upcoming Frontier system. It was intended to help prepare and test applications and systems for the Frontier system."}, {"QUESTION": "Describe the node interconnect and storage capabilities of Spock.", "ANSWER": "Spock nodes were connected using Slingshot-10, providing a node injection bandwidth of 12.5 GB/s. It was also connected to an IBM Spectrum Scale\u2122 filesystem providing 250 PB of storage capacity with a peak write speed of 2.5 TB/s. Additionally, Spock had access to the center-wide NFS-based filesystem for user and project home areas."}, {"QUESTION": "What is the recommended compiler for use on Spock and why?", "ANSWER": "It is highly recommended to use the Cray compiler wrappers (cc, CC, and ftn) on Spock because they automatically add include paths and link in libraries for Cray software, simplifying the compilation process and ensuring compatibility with the system's requirements."}, {"QUESTION": "How does the Lmod module system assist users on Spock?", "ANSWER": "The Lmod module system, a Lua-based module system, assists users on Spock by managing changes to shell environment variables like PATH, LD_LIBRARY_PATH, and PKG_CONFIG_PATH. It allows users to alter the software available in their shell environment dynamically, preventing conflicts between package and version combinations."}, {"QUESTION": "Explain how Slurm is used to manage jobs on Spock.", "ANSWER": "Slurm is the workload manager used to submit, run, and monitor jobs on Spock's compute nodes. It provides several methods for job submission, including batch scripts, interactive jobs, and single-command submissions. Slurm commands like sbatch, salloc, and srun are used to manage these processes."}, {"QUESTION": "What are the two Slurm partitions available on Spock, and what are their characteristics?", "ANSWER": "Spock's compute nodes are separated into two Slurm partitions: one for CAAR projects with 24 compute nodes, and one for ECP projects with 12 compute nodes. Each user can have 1 running and 1 eligible job at a time on both partitions, with no limit on the number of jobs submitted for CAAR and up to 5 jobs submitted for ECP."}, {"QUESTION": "How can users map processes and threads to CPUs and GPUs on Spock?", "ANSWER": "Users can map processes (e.g., MPI ranks) and threads (e.g., OpenMP threads) to CPUs and GPUs using Slurm options. The srun command, along with options like --gpus-per-task and --gpu-bind, allows users to control the distribution of these processes and threads across the available hardware resources."}, {"QUESTION": "What steps are involved in using NVMe devices on Spock?", "ANSWER": "To use NVMe devices on Spock, users must request access during job allocation using the -C nvme option with sbatch, salloc, or srun. Once granted, the devices can be accessed at /mnt/bb/<userid>. Users are responsible for moving data to/from the NVMe before and after their jobs."}, {"QUESTION": "What are the peak performance capabilities of the AMD MI100 GPUs in Spock for different precision levels?", "ANSWER": "The AMD MI100 GPUs in Spock have a peak performance of up to 11.5 TFLOPS for double-precision calculations used in modeling and simulation, and up to 184.6 TFLOPS for half-precision calculations used in machine learning and data analytics."}, {"QUESTION": "What is the purpose of the Cray PrgEnv-<compiler> modules on Spock?", "ANSWER": "The Cray PrgEnv-<compiler> modules on Spock are used to load compatible components of a specific compiler toolchain, including the specified compiler, MPI, LibSci, and other libraries. These modules also define a set of compiler wrappers that automatically add include paths and link in libraries for Cray software."}, {"QUESTION": "How does Spock handle GPU-aware MPI communication?", "ANSWER": "Spock uses Cray's MPICH implementation, which is GPU-aware, allowing GPU buffers to be passed directly to MPI calls. This requires specific modules, environment variables, and headers/libraries to be set, ensuring efficient communication between GPUs and CPU processes."}, {"QUESTION": "What are the storage access options available on Spock for data archiving?", "ANSWER": "Spock does not have direct access to the center\u2019s Nearline archival storage system (Kronos), but users can log in to the moderate DTNs to move data to/from Kronos or use the 'OLCF Kronos' Globus collection for data archiving purposes."}, {"QUESTION": "What should users do if they encounter problems while running programs on Spock?", "ANSWER": "If users encounter problems while running programs on Spock, they are encouraged to submit a ticket by emailing help@olcf.ornl.gov for assistance."}, {"QUESTION": "Explain the role of the Infinity Fabric in the Spock system.", "ANSWER": "The Infinity Fabric in the Spock system connects the GPUs in an all-to-all arrangement, allowing for a peak device-to-device bandwidth of 46+46 GB/s. This ensures efficient data transfer between GPUs, enhancing the overall performance of parallel computations."}]}